Sys.setenv(LANGUAGE = "en")
options(stringsAsFactors = FALSE)

# 加载包
library(dplyr)
library(tidyverse)
library(data.table)
library(tgp)
library(snowfall)        # 并行计算支持
library(seqinr)          # 用于序列分析
library(plyr)            # 数据处理工具
library(randomForestSRC) # 随机森林
library(glmnet)          # 广义线性模型
library(plsRglm)         # 偏最小二乘回归
library(gbm)             # 梯度提升机
library(mboost)          # 增强模型
library(e1071)           # SVM等算法
library(BART)            # 贝叶斯加法回归树
library(MASS)            # 包含广泛应用的统计方法
library(xgboost)         # 极端梯度提升模型
library(caret)           # 机器学习算法训练和测试
library(ggplot2)
library(pheatmap)        #画热图
library(ComplexHeatmap)  # 复杂热图绘制
library(RColorBrewer)    # 颜色选择
library(pROC)            # ROC曲线工具
library(circlize)
##设置路径
getwd()
###一定要设置自己的路径
setwd("/Volumes/927/ML/10")



############################################################
###  线性模型1  Lasso 回归  L1 正则逻辑回归（α=1 的 Elastic Net）
# 定义用于运行Lasso正则化线性模型的函数
RunLasso <- function(Train_set, Train_label, mode, classVar){
  RunEnet(Train_set, Train_label, mode, classVar, alpha = 1)
}

###  线性模型2  Ridge 回归   L2 正则逻辑回归（α=0 的 Elastic Net）
# 定义用于运行Ridge正则化线性模型的函数
RunRidge <- function(Train_set, Train_label, mode, classVar){
  RunEnet(Train_set, Train_label, mode, classVar, alpha = 0)
}

###  线性模型3  RunEnet	Elastic Net 弹性网络回归算法  属于正则化线性模型的函数

RunEnet <- function(Train_set, Train_label, mode, classVar, alpha){
  # 使用交叉验证找到最优的正则化参数
  cv.fit = cv.glmnet(x = Train_set,
                     y = Train_label[[classVar]],
                     family = "binomial", alpha = alpha, nfolds = 10)
  # 建立最终模型
  fit = glmnet(x = Train_set,
               y = Train_label[[classVar]],
               family = "binomial", alpha = alpha, lambda = cv.fit$lambda.min)
  fit$subFeature = colnames(Train_set)
  if (mode == "Model") return(fit)
  if (mode == "Variable") return(ExtractVar(fit))
}

###  线性模型4  Stepglm（Stepwise GLM）逐步广义线性回归
# RunStepglm	逐步广义线性模型（Stepwise GLM）  
RunStepglm <- function(Train_set, Train_label, mode, classVar, direction){
  # 使用glm函数和step函数逐步选择模型
  fit <- step(glm(formula = Train_label[[classVar]] ~ .,
                  family = "binomial", 
                  data = as.data.frame(Train_set)),
              direction = direction, trace = 0)
  fit$subFeature = colnames(Train_set)
  if (mode == "Model") return(fit)
  if (mode == "Variable") return(ExtractVar(fit))
}

###  线性模型5  PLSRGLM--偏最小二乘回归和广义线性模型
# 定义用于运行偏最小二乘回归和广义线性模型的函数
RunplsRglm <- function(Train_set, Train_label, mode, classVar){
  # 使用交叉验证评估模型参数
  cv.plsRglm.res = cv.plsRglm(formula = Train_label[[classVar]] ~ ., 
                              data = as.data.frame(Train_set),
                              nt=10, verbose = FALSE)
  # 建立PLSRGLM模型
  fit <- plsRglm(Train_label[[classVar]], 
                 as.data.frame(Train_set), 
                 modele = "pls-glm-logistic",
                 verbose = F, sparse = T)
  fit$subFeature = colnames(Train_set)
  if (mode == "Model") return(fit)
  if (mode == "Variable") return(ExtractVar(fit))
}

## 线性判别模型6   梯度增强线性模型--glmBoost（Generalized Linear Model Boosting）
# 定义用于运行梯度提升机的函数
RunglmBoost <- function(Train_set, Train_label, mode, classVar){
  # 准备数据，将类变量和训练集绑定
  data <- cbind(Train_set, Train_label[classVar])
  data[[classVar]] <- as.factor(data[[classVar]])
  # 建立初步的GLMBoost模型
  fit <- glmboost(eval(parse(text = paste(classVar, "~."))),
                  data = data,
                  family = Binomial())
  # 使用交叉验证调整模型停止迭代的次数
  cvm <- cvrisk(fit, papply = lapply,
                folds = cv(model.weights(fit), type = "kfold"))
  fit <- glmboost(eval(parse(text = paste(classVar, "~."))),
                  data = data,
                  family = Binomial(), 
                  control = boost_control(mstop = max(mstop(cvm), 40)))
  fit$subFeature = colnames(Train_set)
  if (mode == "Model") return(fit)
  if (mode == "Variable") return(ExtractVar(fit))
}

## 线性判别模型7 RunLDA：线性判别分析（高斯生成模型推出的线性判别面）
# 定义用于运行线性判别分析的函数
RunLDA <- function(Train_set, Train_label, mode, classVar){
  # 准备数据，将类变量转换为因子类型
  data <- as.data.frame(Train_set)
  data[[classVar]] <- as.factor(Train_label[[classVar]])
  # 使用train函数建立LDA模型
  fit = train(eval(parse(text = paste(classVar, "~."))), 
              data = data, 
              method="lda",
              trControl = trainControl(method = "cv"))
  fit$subFeature = colnames(Train_set)
  if (mode == "Model") return(fit)
  if (mode == "Variable") return(ExtractVar(fit))
}

## 概率生成模型8  朴素贝叶斯（条件独立假设的生成式概率模型）
# 定义用于运行朴素贝叶斯分类器的函数
RunNaiveBayes <- function(Train_set, Train_label, mode, classVar){
  # 准备数据
  data <- cbind(Train_set, Train_label[classVar])
  data[[classVar]] <- as.factor(data[[classVar]])
  # 建立朴素贝叶斯模型
  fit <- naiveBayes(eval(parse(text = paste(classVar, "~."))), 
                    data = data)
  fit$subFeature = colnames(Train_set)
  if (mode == "Model") return(fit)
  if (mode == "Variable") return(ExtractVar(fit))
}

###  树模型9   随机森林
# 定义用于运行随机森林的函数
RunRF <- function(Train_set, Train_label, mode, classVar){
  # 设置随机森林参数，如树的最小节点大小
  rf_nodesize = 5 # 可根据需要调整
  # 准备数据，将类变量转换为因子
  Train_label[[classVar]] <- as.factor(Train_label[[classVar]])
  # 建立随机森林模型
  fit <- rfsrc(formula = formula(paste0(classVar, "~.")),
               data = cbind(Train_set, Train_label[classVar]),
               ntree = 1000, nodesize = rf_nodesize,
               importance = T,
               proximity = T,
               forest = T)
  fit$subFeature = colnames(Train_set)
  if (mode == "Model") return(fit)
  if (mode == "Variable") return(ExtractVar(fit))
}

###  树模型10   梯度提升机
# 定义用于运行梯度提升机的函数
RunGBM <- function(Train_set, Train_label, mode, classVar){
  # 建立初步的GBM模型
  fit <- gbm(formula = Train_label[[classVar]] ~ .,
             data = as.data.frame(Train_set),
             distribution = 'bernoulli',
             n.trees = 10000,
             interaction.depth = 3,
             n.minobsinnode = 10,
             shrinkage = 0.001,
             cv.folds = 10,n.cores = 6)
  # 选择最优的迭代次数
  best <- which.min(fit$cv.error)
  fit <- gbm(formula = Train_label[[classVar]] ~ .,
             data = as.data.frame(Train_set),
             distribution = 'bernoulli',
             n.trees = best,
             interaction.depth = 3,
             n.minobsinnode = 10,
             shrinkage = 0.001, n.cores = 8)
  fit$subFeature = colnames(Train_set)
  if (mode == "Model") return(fit)
  if (mode == "Variable") return(ExtractVar(fit))
}

###  树模型11  梯度提升树的高效实现
# 定义用于运行XGBoost的函数 
RunXGBoost <- function(Train_set, Train_label, mode, classVar){
  # 创建交叉验证折叠
  indexes = createFolds(Train_label[[classVar]], k = 5, list=T)
  # 计算每折的最优模型参数
  CV <- unlist(lapply(indexes, function(pt){
    dtrain = xgb.DMatrix(data = Train_set[-pt, ], 
                         label = Train_label[-pt, ])
    dtest = xgb.DMatrix(data = Train_set[pt, ], 
                        label = Train_label[pt, ])
    watchlist <- list(train=dtrain, test=dtest)
    bst <- xgb.train(data=dtrain, 
                     max.depth=2, eta=1, nthread = 2, nrounds=10, 
                     watchlist=watchlist, 
                     objective = "binary:logistic", verbose = F)
    which.min(bst$evaluation_log$test_logloss)
  }))
  # 使用最常用的轮数建立最终模型
  nround <- as.numeric(names(which.max(table(CV))))
  fit <- xgboost(data = Train_set, 
                 label = Train_label[[classVar]], 
                 max.depth = 2, eta = 1, nthread = 2, nrounds = nround, 
                 objective = "binary:logistic", verbose = F)
  fit$subFeature = colnames(Train_set)
  if (mode == "Model") return(fit)
  if (mode == "Variable") return(ExtractVar(fit))
}

###  支持向量模型12   支持向量机
# 定义用于运行支持向量机的函数
RunSVM <- function(Train_set, Train_label, mode, classVar){
  # 将数据框转换为因子类型，适合SVM模型输入
  data <- as.data.frame(Train_set)
  data[[classVar]] <- as.factor(Train_label[[classVar]])
  # 建立SVM模型
  fit = svm(formula = eval(parse(text = paste(classVar, "~."))),
            data= data, probability = T)
  fit$subFeature = colnames(Train_set)
  if (mode == "Model") return(fit)
  if (mode == "Variable") return(ExtractVar(fit))
}

#################################################################
#################################################################
#################################################################
# 定义一个函数RunML，用于运行机器学习算法
RunML <- function(method, Train_set, Train_label, mode = "Model", classVar){
  # 清理和准备算法名称和参数
  # 去除方法名称中的空格，例如 'Enet [alpha=0.4]' 变为 'Enet[alpha=0.4]'
  method = gsub(" ", "", method) 
  # 从方法名称中提取算法名称，如从 'Enet[alpha=0.4]' 得到 'Enet'
  method_name = gsub("(\\w+)\\[(.+)\\]", "\\1", method)  
  # 从方法名称中提取参数，如从 'Enet[alpha=0.4]' 得到 'alpha=0.4'
  method_param = gsub("(\\w+)\\[(.+)\\]", "\\2", method) 
  # 根据提取的算法名称，准备相应的参数
  method_param = switch(
    EXPR = method_name,
    "Enet" = list("alpha" = as.numeric(gsub("alpha=", "", method_param))),
    "Stepglm" = list("direction" = method_param),
    NULL  # 如果没有匹配到任何名称，返回NULL
  )
  # 输出正在运行的算法和使用的变量数
  message("Run ", method_name, " algorithm for ", mode, "; ",
          method_param, ";",
          " using ", ncol(Train_set), " Variables")
  # 将传入的参数和提取的参数组合成一个新的参数列表
  args = list("Train_set" = Train_set,
              "Train_label" = Train_label,
              "mode" = mode,
              "classVar" = classVar)
  args = c(args, method_param)
  # 使用do.call动态调用相应的算法实现函数
  obj <- do.call(what = paste0("Run", method_name),
                 args = args) 
  # 根据模式，输出不同的信息
  if(mode == "Variable"){
    message(length(obj), " Variables retained;\n")
  }else{message("\n")}
  return(obj)
}
# 定义一个函数用于标准化数据
standarize.fun <- function(indata, centerFlag, scaleFlag) {  
  scale(indata, center=centerFlag, scale=scaleFlag)
}
# 定义一个函数用于批量处理数据的标准化
scaleData <- function(data, cohort = NULL, centerFlags = NULL, scaleFlags = NULL){
  samplename = rownames(data)  # 保存原始样本名称
  # 如果没有指定队列，将所有数据视为一个队列
  if (is.null(cohort)){
    data <- list(data); names(data) = "training"
  }else{
    data <- split(as.data.frame(data), cohort)  # 根据队列分割数据
  }
  # 如果没有提供中心化标志，默认不进行中心化
  if (is.null(centerFlags)){
    centerFlags = F; message("No centerFlags found, set as FALSE")
  }
  # 如果中心化标志是单一值，应用于所有数据
  if (length(centerFlags)==1){
    centerFlags = rep(centerFlags, length(data)); message("set centerFlags for all cohort as ", unique(centerFlags))
  }
  # 如果中心化标志没有命名，按顺序匹配
  if (is.null(names(centerFlags))){
    names(centerFlags) <- names(data); message("match centerFlags with cohort by order\n")
  }
  # 如果没有提供缩放标志，默认不进行缩放
  if (is.null(scaleFlags)){
    scaleFlags = F; message("No scaleFlags found, set as FALSE")
  }
  # 如果缩放标志是单一值，应用于所有数据
  if (length(scaleFlags)==1){
    scaleFlags = rep(scaleFlags, length(data)); message("set scaleFlags for all cohort as ", unique(scaleFlags))
  }
  # 如果缩放标志没有命名，按顺序匹配
  if (is.null(names(scaleFlags))){
    names(scaleFlags) <- names(data); message("match scaleFlags with cohort by order\n")
  }
  centerFlags <- centerFlags[names(data)]; scaleFlags <- scaleFlags[names(data)]
  # 使用mapply函数对每个数据队列应用标准化函数
  outdata <- mapply(standarize.fun, indata = data, centerFlag = centerFlags, scaleFlag = scaleFlags, SIMPLIFY = F)
  # lapply(out.data, function(x) summary(apply(x, 2, var)))
  # 将处理后的数据按原始顺序重新组合
  outdata <- do.call(rbind, outdata)
  outdata <- outdata[samplename, ]
  return(outdata)
}
# 定义一个函数用于从模型中提取重要的变量
ExtractVar <- function(fit){
  Feature <- quiet(switch(
    EXPR = class(fit)[1],
    "lognet" = rownames(coef(fit))[which(coef(fit)[, 1]!=0)], # 从Elastic Net模型中提取非零系数的变量
    "glm" = names(coef(fit)), # 从广义线性模型中提取变量
    "svm.formula" = fit$subFeature, # SVM模型中未进行变量选择，使用所有变量
    "train" = fit$coefnames, # 训练集中使用的变量
    "glmboost" = names(coef(fit)[abs(coef(fit))>0]), # 从GLMBoost模型中提取系数非零的变量
    "plsRglmmodel" = rownames(fit$Coeffs)[fit$Coeffs!=0], # 从PLSRGLM模型中提取系数非零的变量
    "rfsrc" = names(which(fit$importance[,1] > 0.01)), 
    "gbm" = rownames(summary.gbm(fit, plotit = F))[summary.gbm(fit, plotit = F)$rel.inf>0], # 从GBM模型中提取重要的变量
    "xgb.Booster" = fit$subFeature, # XGBoost模型中使用的所有变量
    "naiveBayes" = fit$subFeature # 朴素贝叶斯模型中使用的所有变量
  ))
  # 从提取的变量中移除截距项
  Feature <- setdiff(Feature, c("(Intercept)", "Intercept"))
  return(Feature)
}
# 定义一个函数用于计算预测得分
CalPredictScore <- function(fit, new_data, type = "lp") {
  new_data <- new_data[, fit$subFeature, drop = FALSE]
  RS <- quiet(switch(
    EXPR = class(fit)[1],
    "lognet"      = predict(fit, type = 'response', as.matrix(new_data)),
    "glm"         = predict(fit, type = 'response', as.data.frame(new_data)),
    "svm.formula" = {
      prob <- attr(predict(fit, as.data.frame(new_data), probability = TRUE), "probabilities")
      if ("1" %in% colnames(prob)) prob[, "1"] else prob[, 1]
    },
    "train"       = {
      prob <- predict(fit, new_data, type = "prob")
      if ("1" %in% colnames(prob)) prob[, "1"] else prob[, 1]
    },
    "glmboost"    = predict(fit, type = "response", as.data.frame(new_data)),
    "plsRglmmodel" = predict(fit, type = "response", as.data.frame(new_data)),
    "rfsrc"        = {
      pred <- predict(fit, as.data.frame(new_data))$predicted
      if (is.matrix(pred)) pred[, "1"] else pred  # 有些是向量
    },
    "gbm"         = predict(fit, type = 'response', as.data.frame(new_data)),
    "xgb.Booster" = predict(fit, as.matrix(new_data)),
    "naiveBayes"  = {
      prob <- predict(object = fit, type = "raw", newdata = new_data)
      if ("1" %in% colnames(prob)) prob[, "1"] else prob[, 1]
    }
  ))
  RS <- as.numeric(RS)
  if (length(RS) != nrow(new_data)) {
    warning(paste0(⚠️ Prediction length mismatch in model [", class(fit)[1], "] → ",
                   "RS length: ", length(RS), " vs samples: ", nrow(new_data)))
    RS <- rep(NA, nrow(new_data))
  }
  names(RS) <- rownames(new_data)
  return(RS)
}
# 定义一个函数用于预测类别
PredictClass <- function(fit, new_data){
  new_data <- new_data[, fit$subFeature, drop = FALSE]
  label <- quiet(switch(
    EXPR = class(fit)[1],
    "lognet" = predict(fit, type = 'class', as.matrix(new_data)),
    "glm" = ifelse(predict(fit, type = 'response', as.data.frame(new_data)) > 0.5, "1", "0"),
    "svm.formula" = predict(fit, as.data.frame(new_data), decision.values = TRUE),
    "train" = predict(fit, new_data, type = "raw"),
    "glmboost" = predict(fit, type = "class", as.data.frame(new_data)),
    "plsRglmmodel" = ifelse(predict(fit, type = 'response', as.data.frame(new_data)) > 0.5, "1", "0"),
    "rfsrc" = predict(fit, as.data.frame(new_data))$class,
    "gbm" = ifelse(predict(fit, type = 'response', as.data.frame(new_data)) > 0.5, "1", "0"),
    "xgb.Booster" = ifelse(predict(fit, as.matrix(new_data)) > 0.5, "1", "0"),
    "naiveBayes" = predict(object = fit, type = "class", newdata = new_data)
  ))
  label <- as.character(label)
  if (length(label) != nrow(new_data)) {
    warning(paste0("⚠️ PredictClass: model [", class(fit)[1], "] length mismatch → ",
                   "predicted: ", length(label), " vs actual: ", nrow(new_data)))
    label <- rep(NA, nrow(new_data))
  }
  names(label) <- rownames(new_data)
  return(label)
}
# 定义一个函数用于评估模型性能
RunEval <- function(fit, 
                    Test_set = NULL, 
                    Test_label = NULL, 
                    Train_set = NULL, 
                    Train_label = NULL, 
                    Train_name = NULL,
                    cohortVar = "Cohort",
                    classVar){
  # 检查测试标签中是否存在队列指标
  if(!is.element(cohortVar, colnames(Test_label))) {
    stop(paste0("There is no [", cohortVar, "] indicator, please fill in one more column!"))
  } 
  # 如果提供了训练集和训练标签，将它们与测试集合并
  if((!is.null(Train_set)) & (!is.null(Train_label))) {
    new_data <- rbind.data.frame(Train_set[, fit$subFeature],
                                 Test_set[, fit$subFeature])
    # 如果提供了训练名称，将其作为队列名称
    if(!is.null(Train_name)) {
      Train_label$Cohort <- Train_name
    } else {
      Train_label$Cohort <- "Training"
    }
    # 更新训练标签的列名，包括队列变量和类变量
    colnames(Train_label)[ncol(Train_label)] <- cohortVar
    Test_label <- rbind.data.frame(Train_label[,c(cohortVar, classVar)],
                                   Test_label[,c(cohortVar, classVar)])
    Test_label[,1] <- factor(Test_label[,1], 
                             levels = c(unique(Train_label[,cohortVar]), setdiff(unique(Test_label[,cohortVar]),unique(Train_label[,cohortVar]))))
  } else {
    new_data <- Test_set[, fit$subFeature]
  }
  # 计算预测得分
  RS <- suppressWarnings(as.numeric(CalPredictScore(fit = fit, new_data = new_data)))
  # 准备输出数据，包括预测得分
  Predict.out <- Test_label
  Predict.out$RS <- as.vector(RS)
  # 按队列分组
  Predict.out <- split(x = Predict.out, f = Predict.out[,cohortVar])
  # 计算每个队列的AUC值
  unlist(lapply(Predict.out, function(data){
    # 检查预测值是否为数值类型，且没有全部缺失
    if (!is.numeric(data$RS) || all(is.na(data$RS))) {
      warning("❌ AUC skipped: prediction RS invalid.")
      return(NA)
    }
    # 检查是否包含两个类别
    if (length(unique(na.omit(data[[classVar]]))) < 2) {
      warning("❌ AUC skipped: only one class in this cohort.")
      return(NA)
    }
    # 尝试计算AUC，加入错误捕捉
    tryCatch({
      auc(suppressMessages(roc(response = data[[classVar]], predictor = as.numeric(data$RS))))
    }, error = function(e){
      warning(paste0("❌ AUC failed: ", e$message))
      return(NA)
    })
  }))
}
# 定义一个简单的热图绘制函数
SimpleHeatmap <- function(Cindex_mat, avg_Cindex, 
                          CohortCol, barCol,
                          cellwidth = 1, cellheight = 0.5, 
                          cluster_columns, cluster_rows){
  # 定义列注释
  col_ha = columnAnnotation("Cohort" = colnames(Cindex_mat),
                            col = list("Cohort" = CohortCol),
                            show_annotation_name = F)
  # 定义行注释，包括平均C指数的条形图
  row_ha = rowAnnotation(bar = anno_barplot(avg_Cindex, bar_width = 0.8, border = FALSE,
                                            gp = gpar(fill = barCol, col = NA),
                                            add_numbers = T, numbers_offset = unit(-10, "mm"),
                                            axis_param = list("labels_rot" = 0),
                                            numbers_gp = gpar(fontsize = 9, col = "white"),
                                            width = unit(3, "cm")),
                         show_annotation_name = F)
  # 创建热图
  Heatmap(as.matrix(Cindex_mat), name = "AUC",
          right_annotation = row_ha, 
          top_annotation = col_ha,
          col = c("#4195C1", "#FFFFFF", "#FFBC90"), # 定义颜色，从蓝到红通过白色渐变
          rect_gp = gpar(col = "black", lwd = 1), # 设置边框颜色为黑色
          cluster_columns = cluster_columns, cluster_rows = cluster_rows, # 是否对行和列进行聚类
          show_column_names = FALSE, 
          show_row_names = TRUE,
          row_names_side = "left",
          width = unit(cellwidth * ncol(Cindex_mat) + 2, "cm"),
          height = unit(cellheight * nrow(Cindex_mat), "cm"),
          column_split = factor(colnames(Cindex_mat), levels = colnames(Cindex_mat)), 
          column_title = NULL,
          cell_fun = function(j, i, x, y, w, h, col) { # 在每个格子中添加文本
            grid.text(label = format(Cindex_mat[i, j], digits = 3, nsmall = 3),
                      x, y, gp = gpar(fontsize = 10))
          }
  )
}
# 定义一个函数用于在执行过程中抑制输出
quiet <- function(..., messages=FALSE, cat=FALSE){
  if(!cat){
    sink(tempfile())  # 将输出重定向到临时文件
    on.exit(sink())  # 确保在函数退出时恢复正常输出
  }
  # 根据参数决定是否抑制消息
  out <- if(messages) eval(...) else suppressMessages(eval(...))
  out
}











RunGBM <- function(Train_set, Train_label, mode, classVar){
  # 获取样本量，动态调整参数
  n_samples <- nrow(Train_set)
  n_minobsinnode <- max(2, floor(n_samples * 0.05))  # 至少2个样本/节点，或5%样本量
  interaction_depth <- min(2, floor(log2(n_samples/5)))  # 降低交互深度
  
  # 调整参数适配小数据
  fit <- gbm(formula = Train_label[[classVar]] ~ .,
             data = as.data.frame(Train_set),
             distribution = 'bernoulli',
             n.trees = 500,  # 减少树数量
             interaction.depth = interaction_depth,
             n.minobsinnode = n_minobsinnode,
             shrinkage = 0.01,  # 增大学习率
             cv.folds = min(5, n_samples),  # 减少CV折数
             n.cores = 6)
  
  # 避免空CV结果
  if (length(fit$cv.error) == 0) {
    best <- 100  # 默认树数量
  } else {
    best <- which.min(fit$cv.error)
    best <- max(10, min(best, 500))  # 限制树数量范围
  }
  
  # 重新训练最终模型
  fit <- gbm(formula = Train_label[[classVar]] ~ .,
             data = as.data.frame(Train_set),
             distribution = 'bernoulli',
             n.trees = best,
             interaction.depth = interaction_depth,
             n.minobsinnode = n_minobsinnode,
             shrinkage = 0.01, 
             n.cores = 8)
  
  fit$subFeature = colnames(Train_set)
  if (mode == "Model") return(fit)
  if (mode == "Variable") return(ExtractVar(fit))
}



min.selected.var <- 2  
preTrain.var <- list()  
set.seed(seed = 123)         



#读入训练集表达矩阵，及分组信息（结局事件）
Train_expr <- read.table("Training_expr.txt", header = T, sep = "\t", row.names = 1,check.names = F,stringsAsFactors = F)
Train_class <- read.table("Training_class.txt", header = T, sep = "\t", row.names = 1,check.names = F,stringsAsFactors = F)
#提取共有样本
comsam <- intersect(rownames(Train_class), colnames(Train_expr))
Train_expr <- Train_expr[,comsam]
Train_class <- Train_class[comsam,,drop = F]

#读入训练集表达矩阵，及分组信息（结局事件）
Test_expr <- read.table("Testing_expr.txt", header = T, sep = "\t", row.names = 1,check.names = F,stringsAsFactors = F)
Test_class <- read.table("Testing_class.txt", header = T, sep = "\t", row.names = 1,check.names = F,stringsAsFactors = F)
#提取共有样本
comsam <- intersect(rownames(Test_class), colnames(Test_expr))
Test_expr <- Test_expr[,comsam]
Test_class <- Test_class[comsam,,drop = F]

#提取相同基因
comgene <- intersect(rownames(Train_expr),rownames(Test_expr))
Train_expr <- t(Train_expr[comgene,])
Test_expr <- t(Test_expr[comgene,])

# 对数据进行标准化处理
Train_set = scaleData(data=Train_expr, centerFlags=T, scaleFlags=T) 
# 在标准化后的训练集上添加噪声
set.seed(123)  # 为了可重复性，设置随机种子
noise <- matrix(rnorm(n = nrow(Train_set) * ncol(Train_set), mean = 0, sd = 0.01), 
                nrow = nrow(Train_set), ncol = ncol(Train_set))
Train_set <- Train_set + noise
# 测试集也可以添加噪声，如果希望影响模型在测试集上的表现
Test_set = scaleData(data = Test_expr, cohort = Test_class$Cohort, centerFlags = T, scaleFlags = T)
noise_test <- matrix(rnorm(n = nrow(Test_set) * ncol(Test_set), mean = 0, sd = 0.01), 
                     nrow = nrow(Test_set), ncol = ncol(Test_set))
Test_set <- Test_set + noise_test
names(x = split(as.data.frame(Test_expr), f = Test_class$Cohort))
#Test_set = scaleData(data = Test_expr, cohort = Test_class$Cohort, centerFlags = T, scaleFlags = T)
# 读取机器学习方法列表
methodRT <- read.table("refer.txt", header=T, sep="\t", check.names=F)
methods=methodRT$Model
methods <- gsub("-| ", "", methods) # 清理方法名称中的连字符和空格
# 3.准备运行机器学习模型的参数
classVar = "Type"         # 设置类变量的名称
Variable = colnames(Train_set)
preTrain.method =  strsplit(methods, "\\+") # 分解方法名称中的组合
preTrain.method = lapply(preTrain.method, function(x) rev(x)[-1]) # 反转并移除第一个元素
preTrain.method = unique(unlist(preTrain.method)) # 去除重复的方法名称
###################### 根据训练数据运行机器学习模型 ######################
# 4.使用机器学习方法选择变量
# 初始化保存变量选择结果的列表
preTrain.var <- list()  
# 设置随机种子以保证结果的可重复性
set.seed(seed = 123)         
for (method in preTrain.method){
  preTrain.var[[method]] = RunML(method = method,              # 指定机器学习方法
                                 Train_set = Train_set,        # 提供训练数据
                                 Train_label = Train_class,    # 提供类别标签
                                 mode = "Variable",            # 设置模式为变量选择
                                 classVar = classVar)          # 指定类变量
}
preTrain.var[["simple"]] <- colnames(Train_set) # 将简单模型的变量也保存下来
# 5.使用选定的变量建立机器学习模型
# 初始化保存模型结果的列表
model <- list()   
# 再次设置随机种子
set.seed(seed = 123)      
# 备份原始训练集
Train_set_bk <- Train_set  
# 设置变量选择的最小数目
min.selected.var =2    
for (method in methods) {
  cat(match(method, methods), ":", method, "\n")
  # 拆分 simple+算法 这种组合
  parts <- strsplit(method, "\\+")[[1]]
  if (length(parts) == 1) parts <- c("simple", parts)
  # 1) 拿到这一 combo 选中的变量列表
  vars <- preTrain.var[[ parts[1] ]]
  # 2) 如果选中变量太少，跳过
  if (length(vars) <= min.selected.var) {
    message("  SKIP ", parts[1], " → only ", length(vars), " variables\n")
    next
  }
  # 3) 建立模型
  ts <- Train_set_bk[, vars, drop = FALSE]
  fit <- RunML(method      = parts[2],
               Train_set   = ts,
               Train_label = Train_class,
               mode        = "Model",
               classVar    = classVar)
  # 4) 如果这个模型又回头只剩下太少的变量，也删掉
  if (length(ExtractVar(fit)) <= min.selected.var) {
    message("  DROP ", method, " → only ", length(ExtractVar(fit)), " vars after modelling\n")
  } else {
    model[[ method ]] <- fit
  }
}
# 备份
Train_set <- Train_set_bk
#rm(Train_set_bk)
# 保存训练好的机器学习模型
saveRDS(model, "model.MLmodel.rds")
# 使用保存的模型计算每个样本的风险分数
# 加载机器学习模型
model <- readRDS("model.MLmodel.rds")   
# 加载逻辑回归模型
#model <- readRDS("model.logisticmodel.rds") 
# 获取有效的模型名称
methodsValid <- names(model)                    
# 7.计算预测的风险分数
all(model[[method]]$subFeature %in% colnames(Train_set))  # TRUE?
all(model[[method]]$subFeature %in% colnames(Test_set))   # TRUE?
fit <- model[[1]]
nd <- rbind(Train_set[, fit$subFeature], Test_set[, fit$subFeature])
RS <- CalPredictScore(fit, new_data = nd)
length(RS)
length(rownames(nd))
for (method in methodsValid) {
  cat("Checking:", method, "\n")
  fit <- model[[method]]
  nd <- rbind(Train_set[, fit$subFeature, drop = FALSE],
              Test_set[, fit$subFeature, drop = FALSE])
  tryCatch({
    RS <- CalPredictScore(fit = fit, new_data = nd)
    if (length(RS) != nrow(nd)) {
      cat("⚠️ Length mismatch →", "RS:", length(RS), "vs", "samples:", nrow(nd), "\n")
    } else {
      cat("✅ Passed:", method, "\n")
    }
  }, error = function(e){
    cat("❌ ERROR in", method, "→", e$message, "\n")
  })
}
RS_list <- list()
for (method in methodsValid){
  RS_list[[method]] <- CalPredictScore(fit = model[[method]], new_data = rbind.data.frame(Train_set,Test_set))
}
riskTab=as.data.frame(t(do.call(rbind, RS_list)))
riskTab=cbind(id=row.names(riskTab), riskTab)
write.table(riskTab, "model.riskMatrix.txt", sep="\t", row.names=F, quote=F)
# 使用保存的模型预测每个样本的类别
Class_list <- list()
for (method in methodsValid){
  Class_list[[method]] <- PredictClass(fit = model[[method]], new_data = rbind.data.frame(Train_set,Test_set))
}
Class_mat <- as.data.frame(t(do.call(rbind, Class_list)))
# 如果需要，可以将测试类别和预测结果合并
#Class_mat <- cbind.data.frame(Test_class, Class_mat[rownames(Class_mat),]) # 可以合并更多测试数据的信息
classTab=cbind(id=row.names(Class_mat), Class_mat)
write.table(classTab, "model.classMatrix.txt", sep="\t", row.names=F, quote=F)
# 提取每个有效模型选择的变量
fea_list <- list()
for (method in methodsValid) {
  fea_list[[method]] <- ExtractVar(model[[method]])
}
fea_df <- lapply(model, function(fit){
  data.frame(ExtractVar(fit))
})
fea_df <- do.call(rbind, fea_df)
fea_df$algorithm <- gsub("(.+)\\.(.+$)", "\\1", rownames(fea_df))
colnames(fea_df)[1] <- "features"
write.table(fea_df, file="model.genes.txt", sep = "\t", row.names = F, col.names = T, quote = F)
# 8.计算每个模型的AUC值
table(Test_class$Cohort, Test_class$Type)
AUC_list <- list()
for (method in methodsValid){
  AUC_list[[method]] <- RunEval(fit = model[[method]],      # 使用机器学习模型
                                Test_set = Test_set,        # 提供测试数据
                                Test_label = Test_class,    # 提供测试标签
                                Train_set = Train_set,      # 提供训练数据
                                Train_label = Train_class,  # 提供训练标签
                                Train_name = "Train",       # 指定训练标签
                                cohortVar = "Cohort",       # 指定队列变量
                                classVar = classVar)        # 指定类变量
}
AUC_mat <- do.call(rbind, AUC_list)
aucTab=cbind(Method=row.names(AUC_mat), AUC_mat)
write.table(aucTab, "model.AUCmatrix.txt", sep="\t", row.names=F, quote=F)
# 9. 绘制AUC热图 
# 准备热图的数据
AUC_mat <- read.table("model.AUCmatrix.txt", header=T, sep="\t", check.names=F, row.names=1, stringsAsFactors=F)
# 计算并排序AUC的平均值，用于优化学习模型的表现
avg_AUC <- apply(AUC_mat, 1, mean)
avg_AUC <- sort(avg_AUC, decreasing = T)
AUC_mat <- AUC_mat[names(avg_AUC),]
# 获取最佳模型的变量选择结果
fea_sel <- fea_list[[rownames(AUC_mat)[1]]]
avg_AUC <- as.numeric(format(avg_AUC, digits = 3, nsmall = 3))
# 定义热图的颜色和注释
CohortCol <- brewer.pal(n = ncol(AUC_mat), name = "Paired")
names(CohortCol) <- colnames(AUC_mat)
# 绘制热图
cellwidth = 1; cellheight = 0.5
hm <- SimpleHeatmap(Cindex_mat = AUC_mat,       # AUC矩阵
                    avg_Cindex = avg_AUC,       # AUC平均值
                    CohortCol = CohortCol,      # 数据集的颜色
                    barCol = "steelblue",       # 条形图的颜色
                    cellwidth = cellwidth, cellheight = cellheight,    # 设置单元格的宽度和高度
                    cluster_columns = F, cluster_rows = F)      # 设置是否对列和行进行聚类
# 保存热图为PDF文件
pdf(file="model.AUCheatmap.pdf", width=cellwidth * ncol(AUC_mat) + 6, height=cellheight * nrow(AUC_mat) * 0.45)
draw(hm, heatmap_legend_side="right", annotation_legend_side="right")
dev.off()


#表达矩阵
Test = t(Test_expr[,best.model.gene])

#注意修改数据集名字
unique(Test_class$Cohort)
genesets = "Text"

#分组信息
con = subset(Test_class,outcome == 0 )
treat = subset(Test_class,outcome == 1)
conData=Test[,rownames(con)]
treatData=Test[,rownames(treat)]
Test=cbind(conData, treatData)
conNum=ncol(conData)
treatNum=ncol(treatData)

#修改分组信息
Type=c(rep("Normal",conNum), rep("Disease",treatNum))
my_comparisons=list()
my_comparisons[[1]]=levels(factor(Type))

#差异分析
newGeneLists=c()
outTab=data.frame()

for(i in row.names(Test)){
  rt1=data.frame(expression=Test[i,], Type=Type)
  
  #对差异基因进行可视化，绘制箱线图
  boxplot=ggboxplot(rt1, x="Type", y="expression", color="Type",
                    xlab="",
                    ylab=paste(i, "expression"),
                    legend.title="",
                    palette = c("#00AF50","#F5B700"),
                    add = "jitter")+ 
    stat_compare_means(comparisons = my_comparisons,method = "t.test")
  pdf(file=paste0("diff/",genesets,".diff.",i,".pdf"), width=5, height=4.5)
  print(boxplot)
  dev.off()
}


####ROC####

#首先需读入499-548行
#模型中基因的ROC曲线
#读入模型中的基因
model.gene=read.table("fea_df.txt", header=T, sep="\t", check.names=F)
#最优算法的名字
best.model = "glmBoost+SVM"
best.model.gene = subset(model.gene,algorithm == best.model)[,1]

#训练集
#表达矩阵
Train = t(Train_set[,best.model.gene])

#注意修改数据集名字
genesets = "Traning"

#分组信息
con = subset(Train_class,outcome == 0)
treat = subset(Train_class,outcome == 1)
conData=Train[,rownames(con)]
treatData=Train[,rownames(treat)]
Train=cbind(conData, treatData)
conNum=ncol(conData)
treatNum=ncol(treatData)
y=c(rep(0,conNum), rep(1,treatNum))

#对交集基因进行循环，绘制ROC曲线
for(x in best.model.gene){
  #绘制ROC曲线
  roc1=roc(y, as.numeric(Train[x,]))
  ci1=ci.auc(roc1, method="bootstrap")
  ciVec=as.numeric(ci1)
  pdf(file=paste0("ROC/",genesets,".ROC.",x,".pdf"), width=5, height=5)
  plot(roc1, print.auc=TRUE, col="red", legacy.axes=T, main=x)
  text(0.39, 0.43, paste0("95% CI: ",sprintf("%.03f",ciVec[1]),"-",sprintf("%.03f",ciVec[3])), col="red")
  dev.off()
}

#测试集
#表达矩阵
Test = t(Test_set[,best.model.gene])

#注意修改数据集名字
unique(Test_class$Cohort)
genesets = "Text"

#分组信息
con = subset(Test_class,outcome == 0 )
treat = subset(Test_class,outcome == 1)
conData=Test[,rownames(con)]
treatData=Test[,rownames(treat)]
Test=cbind(conData, treatData)
conNum=ncol(conData)
treatNum=ncol(treatData)
y=c(rep(0,conNum), rep(1,treatNum))

#对交集基因进行循环，绘制ROC曲线
for(x in best.model.gene){
  #绘制ROC曲线
  roc1=roc(y, as.numeric(Test[x,]))
  ci1=ci.auc(roc1, method="bootstrap")
  ciVec=as.numeric(ci1)
  pdf(file=paste0("ROC/",genesets,".ROC.",x,".pdf"), width=5, height=5)
  plot(roc1, print.auc=TRUE, col="red", legacy.axes=T, main=x)
  text(0.39, 0.43, paste0("95% CI: ",sprintf("%.03f",ciVec[1]),"-",sprintf("%.03f",ciVec[3])), col="red")
  dev.off()
}


